#!/usr/bin/env bash
#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/temp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 10 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "The question is:" -custom_init 0 -device_id "2"

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "The question is:" -custom_init 0

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condtion_answer "after_fusion" -init_text "Ask me a question." -custom_init 0

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "I would like to ask you a question." -custom_init 0


#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "Here are a few examples:" -custom_init 10

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 30000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "Here are a few examples:" -custom_init 100

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "Here are a few examples:" -custom_init 500

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/temp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10 -debug "0,2000" -lm_path "output/vqa_lm_model/model.pt" -reward "lv_norm" -num_episodes_test 10 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion"

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 10000 -debug "0,2000" -lm_path "output/vqa_lm_model/model.pt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion"

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 0 -debug "0,2000" -lm_path "output/vqa_lm_model/model.pt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1  -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -alpha_logits 1

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 0 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1  -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -alpha_logits 1

python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 30000 -debug "0,2000" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "Here are a few examples:" -custom_init 100

python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 30000 -debug "0,2000" -lm_path "output/vqa_lm_model/model.pt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion"

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 30000 -debug "0,10" -lm_path "gpt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion" -init_text "Here are a few examples:" -custom_init 100

#python src/scripts/run.py -env "vqa" -max_len 10 -data_path "data/vqa-v2" -out_path "output/RL/VQA_exp" -model "lstm" -update_every 128 -agent "PPO" -K_epochs 20 -eps_clip 0.02 -lr 0.001 -word_emb_size 32 -hidden_size 64 -num_episodes_train 30000 -debug "0,10" -lm_path "output/vqa_lm_model/model.pt" -reward "lv_norm" -num_episodes_test 100 -mask_answers 1 -truncate_mode "proba_thr" -p_th 0.01 -grad_clip 1 -fusion "average" -condition_answer "after_fusion"

