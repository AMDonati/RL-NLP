[main]
data_path = data/vqa-v2
features_path = data/vqa-v2/coco_trainval.lmdb
out_path = output/RL/VQA_exp_size_truncation_smallvocab_maxlen10/temperature_2
model = lstm
num_layers = 1
word_emb_size = 512
hidden_size = 1024
conv_kernel = 1
stride = 2
num_filters = 3
fusion = average
agent = PPO
k_epochs = 20
update_every = 128
entropy_coeff = 0.01
eps_clip = 0.02
optimizer = adam
opt_schedule = None
div_factor = 25
lr = 0.001
grad_clip = 1
policy_path = ../../output/RL/vqa_bleu_sf2_PPO_top_p0.9_adam_0.001_temp5.0_div0.75_step2500_tmin1.0/20210104-080521/model.pth
env = vqa
max_len = 10
gamma = 1
reward = bleu_sf2
reward_path = None
reward_vocab = None
mask_answers = 1
debug = 0,2000
num_questions = 10
diff_reward = 0
condition_answer = after_fusion
min_data = 1
lm_path = output/vqa_lm_model_smallvocab/model.pt
truncate_mode = top_p
num_truncated = 10
p_th = None
top_p = 0.9
temperature = 5
temp_step = 2500
temp_factor = 0.75
temp_min = 1
temp_max = 10
alpha_logits = 0
alpha_decay_rate = 0
epsilon_truncated = 0
epsilon_truncated_rate = 1
is_loss_correction = 1
init_text = Here are a few examples:
custom_init = 100
add_answers = 0
num_episodes_train = 0
num_episodes_test = 100
train_seed = 0
test_seed = 1
resume_training = None
eval_no_trunc = 1
train_metrics = ['return', 'size_valid_actions', 'valid_actions', 'dialog', 'eps_truncation', 'ttr_question', 'sum_probs', 'true_word_rank', 'true_word_prob']
test_metrics = ['return', 'dialog', 'bleu', 'ppl_dialog_lm', 'ttr_question', 'sum_probs', 'ppl', 'lv_norm', 'ttr', 'dialogimage', 'selfbleu']
test_modes = ['test_images']
logger_level = INFO
log_interval = 10
pretrain = 0
device_id = 0
num_diversity = 1
old_policy_path = None

